@Book{c1,
  editor     = {Beutel, Jacob and Kundel, Harold L. and Van Metter, Richard L. and {Society of Photo-optical Instrumentation Engineers}},
  publisher  = {SPIE},
  title      = {Handbook of medical imaging. {Vol}. 1: {Physics} and psychophysics},
  year       = {2000},
  address    = {Bellingham, Wash. (1000 20th St. Bellingham WA 98225-6705 USA)},
  isbn       = {9780819481184},
  note       = {OCLC: 606655327},
  number     = {PM79},
  series     = {{SPIE} {Press} monograph},
  abstract   = {This book examines x-ray imaging physics and reviews linear systems theory and its application to signal and noise propagation. The first half addresses the physics of important imaging modalities now in use: ultrasound, CT, MRI, and the recently emerging flat panel x-ray detectors and their application to mammography. The second half describes the relationship between image quality metrics and visual perception of the diagnostic information carried by medical images},
  annote     = {"SPIE digital library."},
  doi        = {10.1117/3.832716},
  file       = {Library Catalog Link:https\://searchworks.stanford.edu/view/9334447:},
  keywords   = {Diagnostic imaging, Handbooks, manuals, etc, Imaging systems in medicine, Diagnostic Imaging, Health Physics, Image Processing, Computer-Assisted, Psychophysics, Technology, Radiologic},
  shorttitle = {Handbook of medical imaging. {Vol}. 1},
  url        = {https://stanford.idm.oclc.org/login?url=http://dx.doi.org/10.1117/3.832716},
  urldate    = {2022-04-04},
}

@Article{c2,
  author     = {Zou, Kelly H. and Warfield, Simon K. and Bharatha, Aditya and Tempany, Clare M. C. and Kaus, Michael R. and Haker, Steven J. and Wells, William M. and Jolesz, Ferenc A. and Kikinis, Ron},
  journal    = {Academic Radiology},
  title      = {Statistical validation of image segmentation quality based on a spatial overlap index1: scientific reports},
  year       = {2004},
  issn       = {1076-6332, 1878-4046},
  month      = feb,
  number     = {2},
  pages      = {178--189},
  volume     = {11},
  doi        = {10.1016/S1076-6332(03)00671-8},
  file       = {Full Text PDF:http\://www.academicradiology.org/article/S1076633203006718/pdf:application/pdf;PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/14974593:text/html},
  keywords   = {Prostate peripheral zone segmentation, brain segmentation, magnetic resonance imaging (MRI), spatial overlap, Dice similarity coefficient},
  language   = {English},
  pmid       = {14974593},
  publisher  = {Elsevier},
  shorttitle = {Statistical validation of image segmentation quality based on a spatial overlap index1},
  url        = {https://www.academicradiology.org/article/S1076-6332(03)00671-8/fulltext},
  urldate    = {2022-04-04},
}

@Misc{c3,
  author   = {Robert D. Hof},
  month    = apr,
  title    = {Deep {Learning}},
  year     = {2013},
  abstract = {When Ray Kurzweil met with Google CEO Larry Page last July, he wasn’t looking for a job. A respected inventor who’s become a machine-intelligence futurist, Kurzweil wanted to discuss his upcoming book How to Create a Mind. He told Page, who had read an early draft, that he wanted to start a company to develop his…},
  journal  = {MIT Technology Review},
  language = {en},
  url      = {https://www.technologyreview.com/technology/deep-learning/},
  urldate  = {2022-04-04},
}

@InProceedings{c4,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
  year      = {2012},
  publisher = {Curran Associates, Inc.},
  volume    = {25},
  abstract  = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
  file      = {Full Text PDF:https\://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf:application/pdf},
  url       = {https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
  urldate   = {2022-04-04},
}

@Article{c5,
  author   = {Bar, Yaniv and Diamant, I. and Wolf, Lior and Lieberman, S. and Konen, E. and Greenspan, H.},
  journal  = {2015 IEEE 12th International Symposium on Biomedical Imaging (ISBI)},
  title    = {Chest pathology detection using deep learning with non-medical training},
  year     = {2015},
  abstract = {This first-of-its-kind experiment shows that Deep learning with ImageNet, a large scale non-medical image database may be a good substitute to domain specific representations, which are yet to be available, for general medical image recognition tasks. In this work, we examine the strength of deep learning approaches for pathology detection in chest radiographs. Convolutional neural networks (CNN) deep architecture classification approaches have gained popularity due to their ability to learn mid and high level image representations. We explore the ability of CNN learned from a non-medical dataset to identify different types of pathologies in chest x-rays. We tested our algorithm on a 433 image dataset. The best performance was achieved using CNN and GIST features. We obtained an area under curve (AUC) of 0.87-0.94 for the different pathologies. The results demonstrate the feasibility of detecting pathology in chest x-rays using deep learning approaches based on non-medical learning. This is a first-of-its-kind experiment that shows that Deep learning with ImageNet, a large scale non-medical image database may be a good substitute to domain specific representations, which are yet to be available, for general medical image recognition tasks.},
  doi      = {10.1109/ISBI.2015.7163871},
  file     = {Semantic Scholar Link:https\://www.semanticscholar.org/paper/Chest-pathology-detection-using-deep-learning-with-Bar-Diamant/5cc51fb6ecadc853cb4017a43fb644ad1b852bc1:text/html},
}

@Article{c6,
  author     = {Shin, Hoo-Chang and Roth, Holger R. and Gao, Mingchen and Lu, Le and Xu, Ziyue and Nogues, Isabella and Yao, Jianhua and Mollura, Daniel and Summers, Ronald M.},
  journal    = {IEEE transactions on medical imaging},
  title      = {Deep {Convolutional} {Neural} {Networks} for {Computer}-{Aided} {Detection}: {CNN} {Architectures}, {Dataset} {Characteristics} and {Transfer} {Learning}},
  year       = {2016},
  issn       = {1558-254X},
  month      = may,
  number     = {5},
  pages      = {1285--1298},
  volume     = {35},
  abstract   = {Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and deep convolutional neural networks (CNNs). CNNs enable learning data-driven, highly representative, hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.},
  doi        = {10.1109/TMI.2016.2528162},
  file       = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/26886976:text/html},
  keywords   = {Databases, Factual, Diagnosis, Computer-Assisted, Humans, Image Interpretation, Lung Diseases, Interstitial, Lymph Nodes, Neural Networks, Computer, Reproducibility of Results},
  language   = {eng},
  pmcid      = {PMC4890616},
  pmid       = {26886976},
  shorttitle = {Deep {Convolutional} {Neural} {Networks} for {Computer}-{Aided} {Detection}},
}

@Article{c7,
  author   = {Gulshan, Varun and Peng, Lily and Coram, Marc and Stumpe, Martin C. and Wu, Derek and Narayanaswamy, Arunachalam and Venugopalan, Subhashini and Widner, Kasumi and Madams, Tom and Cuadros, Jorge and Kim, Ramasamy and Raman, Rajiv and Nelson, Philip C. and Mega, Jessica L. and Webster, Dale R.},
  journal  = {JAMA},
  title    = {Development and {Validation} of a {Deep} {Learning} {Algorithm} for {Detection} of {Diabetic} {Retinopathy} in {Retinal} {Fundus} {Photographs}},
  year     = {2016},
  issn     = {0098-7484},
  month    = dec,
  number   = {22},
  pages    = {2402--2410},
  volume   = {316},
  abstract = {Deep learning is a family of computational methods that allow an algorithm to program itself by learning from a large set of examples that demonstrate the desired behavior, removing the need to specify rules explicitly. Application of these methods to medical imaging requires further assessment and validation.To apply deep learning to create an algorithm for automated detection of diabetic retinopathy and diabetic macular edema in retinal fundus photographs.A specific type of neural network optimized for image classification called a deep convolutional neural network was trained using a retrospective development data set of 128 175 retinal images, which were graded 3 to 7 times for diabetic retinopathy, diabetic macular edema, and image gradability by a panel of 54 US licensed ophthalmologists and ophthalmology senior residents between May and December 2015. The resultant algorithm was validated in January and February 2016 using 2 separate data sets, both graded by at least 7 US board-certified ophthalmologists with high intragrader consistency.Deep learning–trained algorithm.The sensitivity and specificity of the algorithm for detecting referable diabetic retinopathy (RDR), defined as moderate and worse diabetic retinopathy, referable diabetic macular edema, or both, were generated based on the reference standard of the majority decision of the ophthalmologist panel. The algorithm was evaluated at 2 operating points selected from the development set, one selected for high specificity and another for high sensitivity.The EyePACS-1 data set consisted of 9963 images from 4997 patients (mean age, 54.4 years; 62.2\% women; prevalence of RDR, 683/8878 fully gradable images [7.8\%]); the Messidor-2 data set had 1748 images from 874 patients (mean age, 57.6 years; 42.6\% women; prevalence of RDR, 254/1745 fully gradable images [14.6\%]). For detecting RDR, the algorithm had an area under the receiver operating curve of 0.991 (95\% CI, 0.988-0.993) for EyePACS-1 and 0.990 (95\% CI, 0.986-0.995) for Messidor-2. Using the first operating cut point with high specificity, for EyePACS-1, the sensitivity was 90.3\% (95\% CI, 87.5\%-92.7\%) and the specificity was 98.1\% (95\% CI, 97.8\%-98.5\%). For Messidor-2, the sensitivity was 87.0\% (95\% CI, 81.1\%-91.0\%) and the specificity was 98.5\% (95\% CI, 97.7\%-99.1\%). Using a second operating point with high sensitivity in the development set, for EyePACS-1 the sensitivity was 97.5\% and specificity was 93.4\% and for Messidor-2 the sensitivity was 96.1\% and specificity was 93.9\%.In this evaluation of retinal fundus photographs from adults with diabetes, an algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy. Further research is necessary to determine the feasibility of applying this algorithm in the clinical setting and to determine whether use of the algorithm could lead to improved care and outcomes compared with current ophthalmologic assessment.},
  doi      = {10.1001/jama.2016.17216},
  url      = {https://doi.org/10.1001/jama.2016.17216},
  urldate  = {2022-04-04},
}

@Article{c8,
  author   = {Frid-Adar, Maayan and Diamant, Idit and Klang, Eyal and Amitai, Michal and Goldberger, Jacob and Greenspan, Hayit},
  journal  = {Neurocomputing},
  title    = {{GAN}-based synthetic medical image augmentation for increased {CNN} performance in liver lesion classification},
  year     = {2018},
  issn     = {0925-2312},
  month    = dec,
  pages    = {321--331},
  volume   = {321},
  abstract = {Deep learning methods, and in particular convolutional neural networks (CNNs), have led to an enormous breakthrough in a wide range of computer vision tasks, primarily by using large-scale annotated datasets. However, obtaining such datasets in the medical domain remains a challenge. In this paper, we present methods for generating synthetic medical images using recently presented deep learning Generative Adversarial Networks (GANs). Furthermore, we show that generated medical images can be used for synthetic data augmentation, and improve the performance of CNN for medical image classification. Our novel method is demonstrated on a limited dataset of computed tomography (CT) images of 182 liver lesions (53 cysts, 64 metastases and 65 hemangiomas). We first exploit GAN architectures for synthesizing high quality liver lesion ROIs. Then we present a novel scheme for liver lesion classification using CNN. Finally, we train the CNN using classic data augmentation and our synthetic data augmentation and compare performance. In addition, we explore the quality of our synthesized examples using visualization and expert assessment. The classification performance using only classic data augmentation yielded 78.6\% sensitivity and 88.4\% specificity. By adding the synthetic data augmentation the results increased to 85.7\% sensitivity and 92.4\% specificity. We believe that this approach to synthetic data augmentation can generalize to other medical classification applications and thus support radiologists’ efforts to improve diagnosis.},
  doi      = {10.1016/j.neucom.2018.09.013},
  file     = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/S0925231218310749/pdfft?isDTMRedir=true&download=true:application/pdf},
  keywords = {Image synthesis, Data augmentation, Convolutional neural networks, Generative adversarial network, Deep learning, Liver lesions, Lesion classification},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0925231218310749},
  urldate  = {2022-04-04},
}

@InProceedings{c9,
  author     = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle  = {Medical {Image} {Computing} and {Computer}-{Assisted} {Intervention} – {MICCAI} 2015},
  title      = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
  year       = {2015},
  address    = {Cham},
  editor     = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  pages      = {234--241},
  publisher  = {Springer International Publishing},
  abstract   = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.},
  doi        = {10.1007/978-3-319-24574-4_28},
  file       = {Springer Full Text PDF:https\://link.springer.com/content/pdf/10.1007%2F978-3-319-24574-4_28.pdf:application/pdf},
  isbn       = {9783319245744},
  language   = {en},
  shorttitle = {U-{Net}},
}

@InProceedings{c10,
  author    = {Simonyan, Karen and Zisserman, Andrew},
  booktitle = {3rd {International} {Conference} on {Learning} {Representations}, {ICLR} 2015, {San} {Diego}, {CA}, {USA}, {May} 7-9, 2015, {Conference} {Track} {Proceedings}},
  title     = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
  year      = {2015},
  editor    = {Bengio, Yoshua and LeCun, Yann},
  url       = {http://arxiv.org/abs/1409.1556},
  urldate   = {2022-04-04},
}

@InProceedings{c11,
  author    = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  title     = {Going deeper with convolutions},
  year      = {2015},
  month     = jun,
  note      = {ISSN: 1063-6919},
  pages     = {1--9},
  abstract  = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  doi       = {10.1109/CVPR.2015.7298594},
  issn      = {1063-6919},
  keywords  = {Computer architecture, Convolutional codes, Sparse matrices, Neural networks, Visualization, Object detection, Computer vision},
}

@InProceedings{c12,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  title     = {Deep {Residual} {Learning} for {Image} {Recognition}},
  year      = {2016},
  month     = jun,
  note      = {ISSN: 1063-6919},
  pages     = {770--778},
  abstract  = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  doi       = {10.1109/CVPR.2016.90},
  issn      = {1063-6919},
  keywords  = {Training, Degradation, Complexity theory, Image recognition, Neural networks, Visualization, Image segmentation},
}

@InProceedings{c13,
  author    = {Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q.},
  booktitle = {2017 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
  title     = {Densely {Connected} {Convolutional} {Networks}},
  year      = {2017},
  month     = jul,
  note      = {ISSN: 1063-6919},
  pages     = {2261--2269},
  abstract  = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections-one between each layer and its subsequent layer-our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet.},
  doi       = {10.1109/CVPR.2017.243},
  issn      = {1063-6919},
  keywords  = {Training, Convolution, Network architecture, Convolutional codes, Neural networks, Road transportation},
}

@InProceedings{c14,
  author    = {Lee, Chen-Yu and Xie, Saining and Gallagher, Patrick and Zhang, Zhengyou and Tu, Zhuowen},
  title     = {Deeply-{Supervised} {Nets}},
  year      = {2015},
  month     = feb,
  pages     = {562--570},
  publisher = {PMLR},
  abstract  = {We propose deeply-supervised nets (DSN), a method that simultaneously minimizes classification error and improves the directness and transparency of the hidden layer learning process. We focus our attention on three aspects of traditional convolutional-neural-network-type (CNN-type) architectures:  (1) transparency in the effect intermediate layers have on overall classification;  (2) discriminativeness and robustness of learned features, especially in early layers;  (3) training effectiveness in the face of “vanishing” gradients.  To combat these issues, we introduce “companion” objective functions at each hidden layer, in addition to the overall objective function at the output layer (an integrated strategy distinct from layer-wise pre-training). We also analyze our algorithm using techniques extended from stochastic gradient methods. The advantages provided by our method are evident in our experimental results, showing state-of-the-art performance on MNIST, CIFAR-10, CIFAR-100, and SVHN.},
  file      = {Full Text PDF:http\://proceedings.mlr.press/v38/lee15a.pdf:application/pdf},
  issn      = {1938-7228},
  language  = {en},
  url       = {https://proceedings.mlr.press/v38/lee15a.html},
  urldate   = {2022-04-04},
}

@Article{c15,
  author   = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal  = {Communications of the ACM},
  title    = {Generative adversarial networks},
  year     = {2020},
  issn     = {0001-0782},
  month    = oct,
  number   = {11},
  pages    = {139--144},
  volume   = {63},
  abstract = {Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.},
  doi      = {10.1145/3422622},
  file     = {Full Text PDF:https\://dl.acm.org/doi/pdf/10.1145/3422622:application/pdf},
  url      = {https://doi.org/10.1145/3422622},
  urldate  = {2022-04-04},
}

@Article{c16,
  author     = {Yi, Xin and Walia, Ekta and Babyn, Paul},
  journal    = {Medical Image Analysis},
  title      = {Generative adversarial network in medical imaging: {A} review},
  year       = {2019},
  issn       = {1361-8415},
  month      = dec,
  pages      = {101552},
  volume     = {58},
  abstract   = {Generative adversarial networks have gained a lot of attention in the computer vision community due to their capability of data generation without explicitly modelling the probability density function. The adversarial loss brought by the discriminator provides a clever way of incorporating unlabeled samples into training and imposing higher order consistency. This has proven to be useful in many cases, such as domain adaptation, data augmentation, and image-to-image translation. These properties have attracted researchers in the medical imaging community, and we have seen rapid adoption in many traditional and novel applications, such as image reconstruction, segmentation, detection, classification, and cross-modality synthesis. Based on our observations, this trend will continue and we therefore conducted a review of recent advances in medical imaging using the adversarial training scheme with the hope of benefiting researchers interested in this technique.},
  doi        = {10.1016/j.media.2019.101552},
  file       = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/S1361841518308430/pdfft?isDTMRedir=true&download=true:application/pdf},
  keywords   = {Deep learning, Generative adversarial network, Generative model, Medical imaging, Review},
  language   = {en},
  shorttitle = {Generative adversarial network in medical imaging},
  url        = {https://www.sciencedirect.com/science/article/pii/S1361841518308430},
  urldate    = {2022-04-04},
}

@Article{c18,
  author     = {Dewey, Blake E. and Zhao, Can and Reinhold, Jacob C. and Carass, Aaron and Fitzgerald, Kathryn C. and Sotirchos, Elias S. and Saidha, Shiv and Oh, Jiwon and Pham, Dzung L. and Calabresi, Peter A. and van Zijl, Peter C. M. and Prince, Jerry L.},
  journal    = {Magnetic Resonance Imaging},
  title      = {{DeepHarmony}: {A} deep learning approach to contrast harmonization across scanner changes},
  year       = {2019},
  issn       = {1873-5894},
  month      = dec,
  pages      = {160--170},
  volume     = {64},
  abstract   = {Magnetic resonance imaging (MRI) is a flexible medical imaging modality that often lacks reproducibility between protocols and scanners. It has been shown that even when care is taken to standardize acquisitions, any changes in hardware, software, or protocol design can lead to differences in quantitative results. This greatly impacts the quantitative utility of MRI in multi-site or long-term studies, where consistency is often valued over image quality. We propose a method of contrast harmonization, called DeepHarmony, which uses a U-Net-based deep learning architecture to produce images with consistent contrast. To provide training data, a small overlap cohort (n = 8) was scanned using two different protocols. Images harmonized with DeepHarmony showed significant improvement in consistency of volume quantification between scanning protocols. A longitudinal MRI dataset of patients with multiple sclerosis was also used to evaluate the effect of a protocol change on atrophy calculations in a clinical research setting. The results show that atrophy calculations were substantially and significantly affected by protocol change, whereas such changes have a less significant effect and substantially reduced overall difference when using DeepHarmony. This establishes that DeepHarmony can be used with an overlap cohort to reduce inconsistencies in segmentation caused by changes in scanner protocol, allowing for modernization of hardware and protocol design in long-term studies without invalidating previously acquired data.},
  doi        = {10.1016/j.mri.2019.05.041},
  file       = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/31301354:text/html},
  keywords   = {Atrophy, Brain, Clinical Protocols, Cohort Studies, Deep Learning, Humans, Image Interpretation, Computer-Assisted, Longitudinal Studies, Magnetic Resonance Imaging, Multiple Sclerosis, Reproducibility of Results, Contrast harmonization, Deep learning, Magnetic resonance imaging},
  language   = {eng},
  pmcid      = {PMC6874910},
  pmid       = {31301354},
  shorttitle = {{DeepHarmony}},
}

@InProceedings{c20,
  author     = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhudinov, Ruslan and Zemel, Rich and Bengio, Yoshua},
  title      = {Show, {Attend} and {Tell}: {Neural} {Image} {Caption} {Generation} with {Visual} {Attention}},
  year       = {2015},
  month      = jun,
  pages      = {2048--2057},
  publisher  = {PMLR},
  abstract   = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
  file       = {Full Text PDF:http\://proceedings.mlr.press/v37/xuc15.pdf:application/pdf},
  issn       = {1938-7228},
  language   = {en},
  shorttitle = {Show, {Attend} and {Tell}},
  url        = {https://proceedings.mlr.press/v37/xuc15.html},
  urldate    = {2022-04-04},
}

@InProceedings{c21,
  author    = {Hu, Jie and Shen, Li and Sun, Gang},
  booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
  title     = {Squeeze-and-{Excitation} {Networks}},
  year      = {2018},
  month     = jun,
  note      = {ISSN: 2575-7075},
  pages     = {7132--7141},
  abstract  = {Convolutional neural networks are built upon the convolution operation, which extracts informative features by fusing spatial and channel-wise information together within local receptive fields. In order to boost the representational power of a network, several recent approaches have shown the benefit of enhancing spatial encoding. In this work, we focus on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We demonstrate that by stacking these blocks together, we can construct SENet architectures that generalise extremely well across challenging datasets. Crucially, we find that SE blocks produce significant performance improvements for existing state-of-the-art deep architectures at minimal additional computational cost. SENets formed the foundation of our ILSVRC 2017 classification submission which won first place and significantly reduced the top-5 error to 2.251\%, achieving a 25\% relative improvement over the winning entry of 2016. Code and models are available at https://github.com/hujie-frank/SENet.},
  doi       = {10.1109/CVPR.2018.00745},
  issn      = {2575-7075},
  keywords  = {Computer architecture, Computational modeling, Convolution, Task analysis, Convolutional codes, Adaptation models, Stacking},
}

@InProceedings{c22,
  author    = {Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus},
  title     = {Self-{Attention} {Generative} {Adversarial} {Networks}},
  year      = {2019},
  month     = may,
  pages     = {7354--7363},
  publisher = {PMLR},
  abstract  = {In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN performs better than prior work, boosting the best published Inception score from 36.8 to 52.52 and reducing Fréchet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.},
  file      = {Full Text PDF:http\://proceedings.mlr.press/v97/zhang19d/zhang19d.pdf:application/pdf},
  issn      = {2640-3498},
  language  = {en},
  url       = {https://proceedings.mlr.press/v97/zhang19d.html},
  urldate   = {2022-04-04},
}

@TechReport{c23,
  author     = {Oktay, Ozan and Schlemper, Jo and Le Folgoc, Loic and Lee, Matthew and Heinrich, Mattias and Misawa, Kazunari and Mori, Kensaku and McDonagh, Steven and Y Hammerla, Nils and Kainz, Bernhard and Glocker, Ben and Rueckert, Daniel},
  title      = {Attention {U}-{Net}: {Learning} {Where} to {Look} for the {Pancreas}},
  year       = {2018},
  month      = apr,
  note       = {ADS Bibcode: 2018arXiv180403999O Type: article},
  abstract   = {We propose a novel attention gate (AG) model for medical imaging that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules of cascaded convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN architectures such as the U-Net model with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed Attention U-Net architecture is evaluated on two large CT abdominal datasets for multi-class image segmentation. Experimental results show that AGs consistently improve the prediction performance of U-Net across different datasets and training sizes while preserving computational efficiency. The code for the proposed architecture is publicly available.},
  file       = {Full Text PDF:https\://ui.adsabs.harvard.edu/link_gateway/2018arXiv180403999O/ARTICLE:application/pdf},
  journal    = {arXiv e-prints},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  shorttitle = {Attention {U}-{Net}},
  url        = {https://ui.adsabs.harvard.edu/abs/2018arXiv180403999O},
  urldate    = {2022-04-04},
}

@Article{c24,
  author     = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  journal    = {The Journal of Machine Learning Research},
  title      = {Neural architecture search: a survey},
  year       = {2019},
  issn       = {1532-4435},
  month      = jan,
  number     = {1},
  pages      = {1997--2017},
  volume     = {20},
  abstract   = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
  file       = {Full Text PDF:https\://dl.acm.org/doi/pdf/10.5555/3322706.3361996:application/pdf},
  keywords   = {autoDL, autoML, search space design, search strategy, performance estimation strategy, neural architecture search},
  shorttitle = {Neural architecture search},
}

@InProceedings{c25,
  author     = {Zhu, Zhuotun and Liu, Chenxi and Yang, Dong and Yuille, Alan and Xu, Daguang},
  booktitle  = {2019 {International} {Conference} on {3D} {Vision} ({3DV})},
  title      = {V-{NAS}: {Neural} {Architecture} {Search} for {Volumetric} {Medical} {Image} {Segmentation}},
  year       = {2019},
  month      = sep,
  note       = {ISSN: 2475-7888},
  pages      = {240--248},
  abstract   = {Deep learning algorithms, in particular 2D and 3D fully convolutional neural networks (FCNs), have rapidly become the mainstream methodology for volumetric medical image segmentation. However, 2D convolutions cannot fully leverage the rich spatial information along the third axis, while 3D convolutions suffer from the demanding computation and high GPU memory consumption. In this paper, we propose to automatically search the network architecture tailoring to volumetric medical image segmentation problem. Concretely, we formulate the structure learning as differentiable neural architecture search, and let the network itself choose between 2D, 3D or Pseudo-3D (P3D) convolutions at each layer. We evaluate our method on 3 public datasets, i.e., the NIH Pancreas dataset, the Lung and Pancreas dataset from the Medical Segmentation Decathlon (MSD) Challenge. Our method, named V-NAS, consistently outperforms other state-of-the-arts on the segmentation tasks of both normal organ (NIH Pancreas) and abnormal organs (MSD Lung tumors and MSD Pancreas tumors), which shows the power of chosen architecture. Moreover, the searched architecture on one dataset can be well generalized to other datasets, which demonstrates the robustness and practical use of our proposed method.},
  doi        = {10.1109/3DV.2019.00035},
  issn       = {2475-7888},
  keywords   = {Three-dimensional displays, Computer architecture, Two dimensional displays, Decoding, Image segmentation, Microprocessors, Pancreas, Medical Image Segmentation, Neural Architecture Search},
  shorttitle = {V-{NAS}},
}

@Article{c26,
  author     = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  journal    = {arXiv:1704.04861 [cs]},
  title      = {{MobileNets}: {Efficient} {Convolutional} {Neural} {Networks} for {Mobile} {Vision} {Applications}},
  year       = {2017},
  month      = apr,
  note       = {arXiv: 1704.04861},
  abstract   = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  file       = {arXiv Fulltext PDF:https\://arxiv.org/pdf/1704.04861.pdf:application/pdf},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition},
  shorttitle = {{MobileNets}},
  url        = {http://arxiv.org/abs/1704.04861},
  urldate    = {2022-04-04},
}

@InProceedings{c27,
  author     = {Zhang, Xiangyu and Zhou, Xinyu and Lin, Mengxiao and Sun, Jian},
  booktitle  = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
  title      = {{ShuffleNet}: {An} {Extremely} {Efficient} {Convolutional} {Neural} {Network} for {Mobile} {Devices}},
  year       = {2018},
  month      = jun,
  note       = {ISSN: 2575-7075},
  pages      = {6848--6856},
  abstract   = {We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8\%) than recent MobileNet [12] on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves 13× actual speedup over AlexNet while maintaining comparable accuracy.},
  doi        = {10.1109/CVPR.2018.00716},
  issn       = {2575-7075},
  keywords   = {Convolution, Complexity theory, Computer architecture, Mobile handsets, Computational modeling, Task analysis, Neural networks},
  shorttitle = {{ShuffleNet}},
}

@Article{c28,
  author     = {Feldman, David P and Crutchfield, James P},
  journal    = {Physics Letters A},
  title      = {Measures of statistical complexity: {Why}?},
  year       = {1998},
  issn       = {0375-9601},
  month      = feb,
  number     = {4},
  pages      = {244--252},
  volume     = {238},
  abstract   = {We review several statistical complexity measures proposed over the last decade and a half as general indicators of structure or correlation. Recently, Lòpez-Ruiz, Mancini, and Calbet [Phys. Lett. A 209 (1995) 321] introduced another measure of statistical complexity CLMC that, like others, satisfies the “boundary conditions” of vanishing in the extreme ordered and disordered limits. We examine some properties of CLMC and find that it is neither an intensive nor an extensive thermodynamic variable and that it vanishes exponentially in the thermodynamic limit for all one-dimensional finite-range spin systems. We propose a simple alteration of CLMC that renders it extensive. However, this remedy results in a quantity that is a trivial function of the entropy density and hence of no use as a measure of structure or memory. We conclude by suggesting that a useful “statistical complexity” must not only obey the ordered-random boundary conditions of vanishing, it must also be defined in a setting that gives a clear interpretation to what structures are quantified.},
  doi        = {10.1016/S0375-9601(97)00855-4},
  file       = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/S0375960197008554/pdfft?isDTMRedir=true&download=true:application/pdf},
  keywords   = {Statistical complexity, Excess entropy, Mutual information, Shannon entropy, Kolmogorov complexity},
  language   = {en},
  shorttitle = {Measures of statistical complexity},
  url        = {https://www.sciencedirect.com/science/article/pii/S0375960197008554},
  urldate    = {2022-04-04},
}

@Article{c29,
  author     = {Goldberger, A. L.},
  journal    = {The Lancet},
  title      = {Non-linear dynamics for clinicians: chaos theory, fractals, and complexity at the bedside},
  year       = {1996},
  issn       = {0140-6736},
  month      = may,
  number     = {9011},
  pages      = {1312--1314},
  volume     = {347},
  abstract   = {Recurrence quantification analysis (RQA) is a nonlinear method providing information on the temporal structure of time series. RQA has been extensively used to explore various noisy and nonstationary physiological signals. However, the application of RQA to force signals acquired during voluntary fatiguing contractions performed until exhaustion remain to be investigated. We aimed to explore the sensitivity of the percentage of determinism (DET), an RQA predictability measure, to detect changes of force signal complexity induced by fatigue and recovery. Changes in force signal complexity were compared between women and men to explore the ability of DET measures to detect different fatigue profiles. Nineteen women and nineteen men performed intermittent isometric contractions of knee extensors at 50\% of maximal voluntary contraction (MVC) until exhaustion. Participants performed MVC before, during and after the fatiguing task to assess neuromuscular fatigue. Recovery measurements were performed three minutes after exhaustion. Particular attention has been given to the selection of the input parameters of RQA and to the influence of nonstationarity. A detailed methodology is provided to apply RQA to force signals. At the whole group level, complexity decreased with fatigue then increased after recovery. Greater fatigability of men was associated with a faster loss of complexity (i.e. faster increase of DET) of force signals. After recovery, complexity returned to baseline value only for women. These findings confirm that RQA is suited to explore force signal temporal structure and is able to reveal changes of complexity induced by fatigue and recovery by taking into account sex differences.
It has been hypothesized that resting state networks (RSNs), extracted from resting state functional magnetic resonance imaging (rsfMRI), likely display unique temporal complexity fingerprints, quantified by their multiscale entropy patterns (McDonough and Nashiro, 2014). This is a hypothesis with a potential capacity for developing digital biomarkers of normal brain function, as well as pathological brain dysfunction. Nevertheless, a limitation of McDonough and Nashiro (2014) was that rsfMRI data from only 20 healthy individuals was used for the analysis. To validate this hypothesis in a larger cohort, we used rsfMRI datasets of 987 healthy young adults from the Human Connectome Project (HCP), aged 22-35, each with four 14.4-min rsfMRI recordings and parcellated into 379 brain regions. We quantified multiscale entropy of rsfMRI time series averaged at different cortical and sub-cortical regions. We performed effect-size analysis on the data in 8 RSNs. Given that the morphology of multiscale entropy is affected by the choice of its tolerance parameter (r{\textless}math{\textgreater}{\textless}mi is="true"{\textgreater}r{\textless}/mi{\textgreater}{\textless}/math{\textgreater}) and embedding dimension (m{\textless}math{\textgreater}{\textless}mi is="true"{\textgreater}m{\textless}/mi{\textgreater}{\textless}/math{\textgreater}), we repeated the analyses at multiple values of r{\textless}math{\textgreater}{\textless}mi is="true"{\textgreater}r{\textless}/mi{\textgreater}{\textless}/math{\textgreater} and m{\textless}math{\textgreater}{\textless}mi is="true"{\textgreater}m{\textless}/mi{\textgreater}{\textless}/math{\textgreater} including the values used in McDonough and Nashiro (2014). Our results reinforced high temporal complexity in the default mode and frontoparietal networks. Lowest temporal complexity was observed in the subcortical areas and limbic system. We investigated the effect of temporal resolution (determined by the repetition time TR{\textless}math{\textgreater}{\textless}msub is="true"{\textgreater}{\textless}mi is="true"{\textgreater}T{\textless}/mi{\textgreater}{\textless}mi is="true"{\textgreater}R{\textless}/mi{\textgreater}{\textless}/msub{\textgreater}{\textless}/math{\textgreater}) after downsampling of rsfMRI time series at two rates. At a low temporal resolution, we observed increased entropy and variance across datasets. Test-retest analysis showed that findings were likely reproducible across individuals over four rsfMRI runs, especially when the tolerance parameter r{\textless}math{\textgreater}{\textless}mi is="true"{\textgreater}r{\textless}/mi{\textgreater}{\textless}/math{\textgreater} is equal to 0.5. The results confirmed that the relationship between functional brain connectivity strengths and rsfMRI temporal complexity changes over time scales. Finally, a non-random correlation was observed between temporal complexity of RSNs and fluid intelligence suggesting that complex dynamics of the human brain is an important attribute of high-level brain function.
The COVID-19 epidemic challenges humanity in 2020. It has already taken an enormous number of human lives and had a substantial negative economic impact. Traditional compartmental epidemiological models demonstrated limited ability to predict the scale and dynamics of COVID-19 epidemic in different countries. In order to gain a deeper understanding of its behavior, we turn to chaotic dynamics, which proved fruitful in analyzing previous diseases such as measles. We hypothesize that the unpredictability of the pandemic could be a fundamental property if the disease spread is a chaotic dynamical system. Our mathematical examination of COVID-19 epidemic data in different countries reveals similarity of this dynamic to the chaotic behavior of many dynamics systems, such as logistic maps. We conclude that the data does suggest that the COVID-19 epidemic demonstrates chaotic behavior, which should be taken into account by public policy makers. Furthermore, the scale and behavior of the epidemic may be essentially unpredictable due to the properties of chaotic systems, rather than due to the limited data available for model parameterization.},
  doi        = {10.1016/S0140-6736(96)90948-4},
  file       = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0140673696909484/pdf?md5=3452cd79b01d50544c5515f222ffd95e&pid=1-s2.0-S0140673696909484-main.pdf&isDTMRedir=Y:application/pdf},
  language   = {en},
  shorttitle = {Non-linear dynamics for clinicians},
  url        = {https://www.sciencedirect.com/science/article/pii/S0140673696909484},
  urldate    = {2022-04-04},
}

@Article{c30,
  author   = {Goldberger, Ary L. and Peng, C.-K. and Lipsitz, Lewis A.},
  journal  = {Neurobiology of Aging},
  title    = {What is physiologic complexity and how does it change with aging and disease?},
  year     = {2002},
  issn     = {0197-4580},
  month    = feb,
  number   = {1},
  pages    = {23--26},
  volume   = {23},
  doi      = {10.1016/s0197-4580(01)00266-4},
  file     = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/11755014:text/html},
  keywords = {Aged, Aging, Disease, Humans, Models, Neurological, Nervous System Physiological Phenomena, Non-programmatic},
  language = {eng},
  pmid     = {11755014},
}

@Article{c31,
  author   = {Costa, Madalena and Goldberger, Ary L. and Peng, C.-K.},
  journal  = {Physical Review. E, Statistical, Nonlinear, and Soft Matter Physics},
  title    = {Multiscale entropy analysis of biological signals},
  year     = {2005},
  issn     = {1539-3755},
  month    = feb,
  number   = {2 Pt 1},
  pages    = {021906},
  volume   = {71},
  abstract = {Traditional approaches to measuring the complexity of biological signals fail to account for the multiple time scales inherent in such time series. These algorithms have yielded contradictory findings when applied to real-world datasets obtained in health and disease states. We describe in detail the basis and implementation of the multiscale entropy (MSE) method. We extend and elaborate previous findings showing its applicability to the fluctuations of the human heartbeat under physiologic and pathologic conditions. The method consistently indicates a loss of complexity with aging, with an erratic cardiac arrhythmia (atrial fibrillation), and with a life-threatening syndrome (congestive heart failure). Further, these different conditions have distinct MSE curve profiles, suggesting diagnostic uses. The results support a general "complexity-loss" theory of aging and disease. We also apply the method to the analysis of coding and noncoding DNA sequences and find that the latter have higher multiscale entropy, consistent with the emerging view that so-called "junk DNA" sequences contain important biological information.},
  doi      = {10.1103/PhysRevE.71.021906},
  file     = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/15783351:text/html},
  keywords = {Adult, Aged, Algorithms, Atrial Fibrillation, Computer Simulation, Diagnosis, Computer-Assisted, Electrocardiography, Ambulatory, Entropy, Female, Heart Failure, Humans, Male, Middle Aged, Models, Biological, Chemical, Reproducibility of Results, Sensitivity and Specificity, Sequence Analysis, DNA},
  language = {eng},
  pmid     = {15783351},
}

@Article{c32,
  author   = {Costa, Madalena and Goldberger, Ary L. and Peng, C.-K.},
  journal  = {Physical Review Letters},
  title    = {Multiscale entropy analysis of complex physiologic time series},
  year     = {2002},
  issn     = {0031-9007},
  month    = aug,
  number   = {6},
  pages    = {068102},
  volume   = {89},
  abstract = {There has been considerable interest in quantifying the complexity of physiologic time series, such as heart rate. However, traditional algorithms indicate higher complexity for certain pathologic processes associated with random outputs than for healthy dynamics exhibiting long-range correlations. This paradox may be due to the fact that conventional algorithms fail to account for the multiple time scales inherent in healthy physiologic dynamics. We introduce a method to calculate multiscale entropy (MSE) for complex time series. We find that MSE robustly separates healthy and pathologic groups and consistently yields higher values for simulated long-range correlated noise compared to uncorrelated noise.},
  doi      = {10.1103/PhysRevLett.89.068102},
  file     = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/12190613:text/html},
  keywords = {Algorithms, Disease, Entropy, Heart Failure, Heart Rate, Humans, Models, Biological, Pathology, Physiology, Time Factors, Non-programmatic},
  language = {eng},
  pmid     = {12190613},
}

@Article{c33,
  author   = {Silva, Luiz Eduardo Virgilio and Cabella, Brenno Caetano Troca and Neves, Ubiraci Pereira da Costa and Murta Junior, Luiz Otavio},
  journal  = {Physica A: Statistical Mechanics and its Applications},
  title    = {Multiscale entropy-based methods for heart rate variability complexity analysis},
  year     = {2015},
  issn     = {0378-4371},
  month    = mar,
  pages    = {143--152},
  volume   = {422},
  abstract = {Physiologic complexity is an important concept to characterize time series from biological systems, which associated to multiscale analysis can contribute to comprehension of many complex phenomena. Although multiscale entropy has been applied to physiological time series, it measures irregularity as function of scale. In this study we purpose and evaluate a set of three complexity metrics as function of time scales. Complexity metrics are derived from nonadditive entropy supported by generation of surrogate data, i.e. SDiffqmax, qmax and qzero. In order to access accuracy of proposed complexity metrics, receiver operating characteristic (ROC) curves were built and area under the curves was computed for three physiological situations. Heart rate variability (HRV) time series in normal sinus rhythm, atrial fibrillation, and congestive heart failure data set were analyzed. Results show that proposed metric for complexity is accurate and robust when compared to classic entropic irregularity metrics. Furthermore, SDiffqmax is the most accurate for lower scales, whereas qmax and qzero are the most accurate when higher time scales are considered. Multiscale complexity analysis described here showed potential to assess complex physiological time series and deserves further investigation in wide context.},
  doi      = {10.1016/j.physa.2014.12.011},
  file     = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/S0378437114010462/pdfft?isDTMRedir=true&download=true:application/pdf},
  keywords = {Nonadditive statistics, Tsallis entropy, Multiscale entropy, Complexity, Heart rate variability},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/S0378437114010462},
  urldate  = {2022-04-04},
}

@Article{c34,
  author     = {Silva, Luiz Eduardo Virgilio and Lataro, Renata Maria and Castania, Jaci Airton and da Silva, Carlos Alberto Aguiar and Valencia, Jose Fernando and Murta, Luiz Otavio and Salgado, Helio Cesar and Fazan, Rubens and Porta, Alberto},
  journal    = {American Journal of Physiology. Regulatory, Integrative and Comparative Physiology},
  title      = {Multiscale entropy analysis of heart rate variability in heart failure, hypertensive, and sinoaortic-denervated rats: classical and refined approaches},
  year       = {2016},
  issn       = {1522-1490},
  month      = jul,
  number     = {1},
  pages      = {R150--156},
  volume     = {311},
  abstract   = {The analysis of heart rate variability (HRV) by nonlinear methods has been gaining increasing interest due to their ability to quantify the complexity of cardiovascular regulation. In this study, multiscale entropy (MSE) and refined MSE (RMSE) were applied to track the complexity of HRV as a function of time scale in three pathological conscious animal models: rats with heart failure (HF), spontaneously hypertensive rats (SHR), and rats with sinoaortic denervation (SAD). Results showed that HF did not change HRV complexity, although there was a tendency to decrease the entropy in HF animals. On the other hand, SHR group was characterized by reduced complexity at long time scales, whereas SAD animals exhibited a smaller short- and long-term irregularity. We propose that short time scales (1 to 4), accounting for fast oscillations, are more related to vagal and respiratory control, whereas long time scales (5 to 20), accounting for slow oscillations, are more related to sympathetic control. The increased sympathetic modulation is probably the main reason for the lower entropy observed at high scales for both SHR and SAD groups, acting as a negative factor for the cardiovascular complexity. This study highlights the contribution of the multiscale complexity analysis of HRV for understanding the physiological mechanisms involved in cardiovascular regulation.},
  doi        = {10.1152/ajpregu.00076.2016},
  file       = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/27225948:text/html},
  keywords   = {Animals, Autonomic Nervous System, Denervation, Entropy, Heart Failure, Heart Rate, Hypertension, Male, Rats, Inbred SHR, Inbred WKY, Wistar, Respiratory Mechanics, Sinus of Valsalva, Vagus Nerve, autonomic nervous system, baroreflex, cardiovascular control, complexity, heart failure, heart rate variability, hypertension, refined multiscale entropy},
  language   = {eng},
  pmid       = {27225948},
  shorttitle = {Multiscale entropy analysis of heart rate variability in heart failure, hypertensive, and sinoaortic-denervated rats},
}

@Article{c35,
  author     = {Silva, Luiz E. V. and Duque, Juliano J. and Felipe, Joaquim C. and Murta Jr, Luiz O. and Humeau-Heurtier, Anne},
  journal    = {Signal Processing},
  title      = {Two-dimensional multiscale entropy analysis: {Applications} to image texture evaluation},
  year       = {2018},
  issn       = {0165-1684},
  month      = jun,
  pages      = {224--232},
  volume     = {147},
  abstract   = {Complexity measures, defined as measures of irregularity over time scales, are the subject of a growing number of studies as the information they reveal can find utility in a large field of applications. One of the most popular complexity measures is the multiscale entropy. Nevertheless, more and more algorithms dedicated to complexity analyses are proposed to improve the existing ones. However, such measures are available only for one-dimensional time series. For bidimensional data (images), no equivalent algorithm has been proposed to analyze irregularity over spatial scales. We herein introduce a new framework that extends the one-dimensional multiscale entropy (MSE1D) to the bidimensional case (MSE2D). Moreover, a variant of MSE2D is also ModMSE2D). The two new algorithms are tested as new texture analysis frameworks. They are applied to simulated and real data. Our results show that, compared with other existing texture analysis algorithms, MSE2D and ModMSE2D are suitable and powerful tools for image analysis and classification according to their texture patterns. While MSE2D is computationally faster than ModMSE2D, ModMSE2D is more robust to small image sizes. The two methods present interesting performances, and can be as useful as their unidimensional versions in two-dimensional applications.},
  doi        = {10.1016/j.sigpro.2018.02.004},
  file       = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/S0165168418300513/pdfft?isDTMRedir=true&download=true:application/pdf},
  keywords   = {Multiscale entropy, Image processing, Texture analysis, Complexity, Irregularity},
  language   = {en},
  shorttitle = {Two-dimensional multiscale entropy analysis},
  url        = {https://www.sciencedirect.com/science/article/pii/S0165168418300513},
  urldate    = {2022-04-04},
}

@InProceedings{c36,
  author     = {Abadi, Martin and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and Kudlur, Manjunath and Levenberg, Josh and Monga, Rajat and Moore, Sherry and Murray, Derek G. and Steiner, Benoit and Tucker, Paul and Vasudevan, Vijay and Warden, Pete and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  booktitle  = {Proceedings of the 12th {USENIX} conference on {Operating} {Systems} {Design} and {Implementation}},
  title      = {{TensorFlow}: a system for large-scale machine learning},
  year       = {2016},
  address    = {USA},
  month      = nov,
  pages      = {265--283},
  publisher  = {USENIX Association},
  series     = {{OSDI}'16},
  abstract   = {TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous "parameter server" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.},
  isbn       = {9781931971331},
  shorttitle = {{TensorFlow}},
  urldate    = {2022-04-04},
}

@Article{c37,
  author     = {Lee, Jang Hyung and Kim, Kwang Gi},
  journal    = {Healthcare Informatics Research},
  title      = {Applying {Deep} {Learning} in {Medical} {Images}: {The} {Case} of {Bone} {Age} {Estimation}},
  year       = {2018},
  issn       = {2093-3681},
  month      = jan,
  number     = {1},
  pages      = {86--92},
  volume     = {24},
  abstract   = {OBJECTIVES: A diagnostic need often arises to estimate bone age from X-ray images of the hand of a subject during the growth period. Together with measured physical height, such information may be used as indicators for the height growth prognosis of the subject. We present a way to apply the deep learning technique to medical image analysis using hand bone age estimation as an example.
METHODS: Age estimation was formulated as a regression problem with hand X-ray images as input and estimated age as output. A set of hand X-ray images was used to form a training set with which a regression model was trained. An image preprocessing procedure is described which reduces image variations across data instances that are unrelated to age-wise variation. The use of Caffe, a deep learning tool is demonstrated. A rather simple deep learning network was adopted and trained for tutorial purpose.
RESULTS: A test set distinct from the training set was formed to assess the validity of the approach. The measured mean absolute difference value was 18.9 months, and the concordance correlation coefficient was 0.78.
CONCLUSIONS: It is shown that the proposed deep learning-based neural network can be used to estimate a subject's age from hand X-ray images, which eliminates the need for tedious atlas look-ups in clinical environments and should improve the time and cost efficiency of the estimation process.},
  doi        = {10.4258/hir.2018.24.1.86},
  file       = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/29503757:text/html},
  keywords   = {Bone Age, Deep Learning, Python, Tensorflow, X-ray Imaging},
  language   = {eng},
  pmcid      = {PMC5820091},
  pmid       = {29503757},
  shorttitle = {Applying {Deep} {Learning} in {Medical} {Images}},
}

@Article{c38,
  author     = {Coy, Heidi and Hsieh, Kevin and Wu, Willie and Nagarajan, Mahesh B. and Young, Jonathan R. and Douek, Michael L. and Brown, Matthew S. and Scalzo, Fabien and Raman, Steven S.},
  journal    = {Abdominal Radiology (New York)},
  title      = {Deep learning and radiomics: the utility of {Google} {TensorFlow}™ {Inception} in classifying clear cell renal cell carcinoma and oncocytoma on multiphasic {CT}},
  year       = {2019},
  issn       = {2366-0058},
  month      = jun,
  number     = {6},
  pages      = {2009--2020},
  volume     = {44},
  abstract   = {PURPOSE: Currently, all solid enhancing renal masses without microscopic fat are considered malignant until proven otherwise and there is substantial overlap in the imaging findings of benign and malignant renal masses, particularly between clear cell RCC (ccRCC) and benign oncocytoma (ONC). Radiomics has attracted increased attention for its utility in pre-operative work-up on routine clinical images. Radiomics based approaches have converted medical images into mineable data and identified prognostic imaging signatures that machine learning algorithms can use to construct predictive models by learning the decision boundaries of the underlying data distribution. The TensorFlow™ framework from Google is a state-of-the-art open-source software library that can be used for training deep learning neural networks for performing machine learning tasks. The purpose of this study was to investigate the diagnostic value and feasibility of a deep learning-based renal lesion classifier using open-source Google TensorFlow™ Inception in differentiating ccRCC from ONC on routine four-phase MDCT in patients with pathologically confirmed renal masses.
METHODS: With institutional review board approval for this 1996 Health Insurance Portability and Accountability Act compliant retrospective study and a waiver of informed consent, we queried our institution's pathology, clinical, and radiology databases for histologically proven cases of ccRCC and ONC obtained between January 2000 and January 2016 scanned with a an intravenous contrast-enhanced four-phase renal mass protocol (unenhanced (UN), corticomedullary (CM), nephrographic (NP), and excretory (EX) phases). To extract features to be used for the machine learning model, the entire renal mass was contoured in the axial plane in each of the four phases, resulting in a 3D volume of interest (VOI) representative of the entire renal mass. We investigated thirteen different approaches to convert the acquired VOI data into a set of images that adequately represented each tumor which was used to train the final layer of the neural network model. Training was performed over 4000 iterations. In each iteration, 90\% of the data were designated as training data and the remaining 10\% served as validation data and a leave-one-out cross-validation scheme was implemented. Accuracy, sensitivity, specificity, positive (PPV) and negative predictive (NPV) values, and CIs were calculated for the classification of the thirteen processing modes.
RESULTS: We analyzed 179 consecutive patients with 179 lesions (128 ccRCC and 51 ONC). The ccRCC cohort had a mean size of 3.8 cm (range 0.8-14.6 cm) and the ONC cohort had a mean lesion size of 3.9 cm (range 1.0-13.1 cm). The highest specificity and PPV (52.9\% and 80.3\%, respectively) were achieved in the EX phase when we analyzed the single mid-slice of the tumor in the axial, coronal and sagittal plane, and when we increased the number of mid-slices of the tumor to three, with an accuracy of 75.4\%, which also increased the sensitivity to 88.3\% and the PPV to 79.6\%. Using the entire tumor volume also showed that classification performance was best in the EX phase with an accuracy of 74.4\%, a sensitivity of 85.8\% and a PPV of 80.1\%. When the entire tumor volume, plus mid-slices from all phases and all planes presented as tiled images, were submitted to the final layer of the neural network we achieved a PPV of 82.5\%.
CONCLUSIONS: The best classification result was obtained in the EX phase among the thirteen classification methods tested. Our proof of concept study is the first step towards understanding the utility of machine learning in the differentiation of ccRCC from ONC on routine CT images. We hope this could lead to future investigation into the development of a multivariate machine learning model which may augment our ability to accurately predict renal lesion histology on imaging.},
  doi        = {10.1007/s00261-019-01929-0},
  file       = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/30778739:text/html},
  keywords   = {Adenoma, Oxyphilic, Adult, Aged, 80 and over, Algorithms, Carcinoma, Renal Cell, Contrast Media, Deep Learning, Diagnosis, Differential, Female, Humans, Iohexol, Kidney Neoplasms, Male, Middle Aged, Multidetector Computed Tomography, Radiographic Image Interpretation, Computer-Assisted, Retrospective Studies, Sensitivity and Specificity, Software, Clear cell renal cell carcinoma, Deep learning, Multiphasic CT, Oncocytoma, Radiogenomics, Radiomics},
  language   = {eng},
  pmid       = {30778739},
  shorttitle = {Deep learning and radiomics},
}

@Article{c39,
  author   = {López-Ruiz, R. and Mancini, H. L. and Calbet, X.},
  journal  = {Physics Letters A},
  title    = {A statistical measure of complexity},
  year     = {1995},
  issn     = {0375-9601},
  month    = dec,
  number   = {5},
  pages    = {321--326},
  volume   = {209},
  abstract = {A measure of complexity based on a probabilistic description of physical systems is proposed. This measure incorporates the main features of the intuitive notion of such a magnitude. It can be applied to many physical situations and to different descriptions of a given system. Moreover, the calculation of its value does not require a considerable computational effort in many cases of physical interest.},
  doi      = {10.1016/0375-9601(95)00867-5},
  file     = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/abs/pii/0375960195008675/pdfft?isDTMRedir=true&download=true:application/pdf},
  language = {en},
  url      = {https://www.sciencedirect.com/science/article/pii/0375960195008675},
  urldate  = {2022-04-04},
}

@Article{MEDMNIST,
  author     = {Yang, Jiancheng and Shi, Rui and Ni, Bingbing},
  journal    = {2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI)},
  title      = {{MedMNIST} {Classification} {Decathlon}: {A} {Lightweight} {AutoML} {Benchmark} for {Medical} {Image} {Analysis}},
  year       = {2021},
  month      = apr,
  note       = {arXiv: 2010.14925},
  pages      = {191--195},
  abstract   = {We present MedMNIST, a collection of 10 pre-processed medical open datasets. MedMNIST is standardized to perform classification tasks on lightweight 28x28 images, which requires no background knowledge. Covering the primary data modalities in medical image analysis, it is diverse on data scale (from 100 to 100,000) and tasks (binary/multi-class, ordinal regression and multi-label). MedMNIST could be used for educational purpose, rapid prototyping, multi-modal machine learning or AutoML in medical image analysis. Moreover, MedMNIST Classification Decathlon is designed to benchmark AutoML algorithms on all 10 datasets; We have compared several baseline methods, including open-source or commercial AutoML tools. The datasets, evaluation code and baseline methods for MedMNIST are publicly available at https://medmnist.github.io/.},
  annote     = {Comment: ISBI 2021. Code and dataset are available at https://medmnist.github.io/},
  doi        = {10.1109/ISBI48211.2021.9434062},
  file       = {arXiv Fulltext PDF:https\://arxiv.org/pdf/2010.14925.pdf:application/pdf},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
  shorttitle = {{MedMNIST} {Classification} {Decathlon}},
  url        = {http://arxiv.org/abs/2010.14925},
  urldate    = {2022-04-04},
}

@InProceedings{c40,
  author     = {Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald M.},
  title      = {{ChestX}-{Ray8}: {Hospital}-{Scale} {Chest} {X}-{Ray} {Database} and {Benchmarks} on {Weakly}-{Supervised} {Classification} and {Localization} of {Common} {Thorax} {Diseases}},
  year       = {2017},
  month      = jul,
  pages      = {3462--3471},
  publisher  = {IEEE Computer Society},
  abstract   = {The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely ChestX-ray8, which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based reading chest X-rays (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems.},
  doi        = {10.1109/CVPR.2017.369},
  file       = {Full Text PDF:https\://www.computer.org/csdl/api/v1/csdl/proceedings/download-article/12OmNx8wTk9/pdf:application/pdf},
  isbn       = {9781538604571},
  issn       = {1063-6919},
  language   = {English},
  shorttitle = {{ChestX}-{Ray8}},
  url        = {https://www.computer.org/csdl/proceedings-article/cvpr/2017/0457d462/12OmNx8wTk9},
  urldate    = {2022-04-04},
}

@Article{c41,
  author     = {Clark, Kenneth and Vendt, Bruce and Smith, Kirk and Freymann, John and Kirby, Justin and Koppel, Paul and Moore, Stephen and Phillips, Stanley and Maffitt, David and Pringle, Michael and Tarbox, Lawrence and Prior, Fred},
  journal    = {Journal of Digital Imaging},
  title      = {The {Cancer} {Imaging} {Archive} ({TCIA}): {Maintaining} and {Operating} a {Public} {Information} {Repository}},
  year       = {2013},
  issn       = {0897-1889},
  month      = dec,
  number     = {6},
  pages      = {1045--1057},
  volume     = {26},
  abstract   = {The National Institutes of Health have placed significant emphasis on sharing of research data to support secondary research. Investigators have been encouraged to publish their clinical and imaging data as part of fulfilling their grant obligations. Realizing it was not sufficient to merely ask investigators to publish their collection of imaging and clinical data, the National Cancer Institute (NCI) created the open source National Biomedical Image Archive software package as a mechanism for centralized hosting of cancer related imaging. NCI has contracted with Washington University in Saint Louis to create The Cancer Imaging Archive (TCIA)—an open-source, open-access information resource to support research, development, and educational initiatives utilizing advanced medical imaging of cancer. In its first year of operation, TCIA accumulated 23 collections (3.3 million images). Operating and maintaining a high-availability image archive is a complex challenge involving varied archive-specific resources and driven by the needs of both image submitters and image consumers. Quality archives of any type (traditional library, PubMed, refereed journals) require management and customer service. This paper describes the management tasks and user support model for TCIA.},
  doi        = {10.1007/s10278-013-9622-7},
  file       = {PubMed Central Link:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC3824915/:text/html},
  pmcid      = {PMC3824915},
  pmid       = {23884657},
  shorttitle = {The {Cancer} {Imaging} {Archive} ({TCIA})},
  url        = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3824915/},
  urldate    = {2022-04-04},
}

@Article{c42,
  author     = {Armato, Samuel G. and McLennan, Geoffrey and Meyer, Charles R. and Reeves, Anthony P. and McNitt-Gray, Michael F. and Croft, Barbara Y. and Clarke, Laurence P.},
  journal    = {Clinical pharmacology and therapeutics},
  title      = {The {Reference} {Image} {Database} to {Evaluate} {Response} to {Therapy} in {Lung} {Cancer} ({RIDER}) {Project}: {A} {Resource} for the {Development} of {Change} {Analysis} {Software}},
  year       = {2008},
  issn       = {0009-9236},
  month      = oct,
  number     = {4},
  pages      = {448--456},
  volume     = {84},
  abstract   = {Critical to the clinical evaluation of effective novel therapies for lung cancer is the early and accurate determination of tumor response, which requires an understanding of the sources of uncertainty in tumor measurements and subsequent attempts to minimize their impact on the assessment of the agent. The Reference Image Database to Evaluate Response (RIDER) project seeks to develop a consensus approach to the optimization and benchmarking of software tools for the assessment of tumor response to therapy and to provide a publicly available database of serial images acquired during lung cancer drug and radiation therapy trials. Images of phantoms and patient images acquired under situations of certain no change in tumor size or biology also will be provided. The RIDER project will create standardized methods for benchmarking software tools to reduce sources of uncertainty in vital clinical decisions such as whether a specific tumor is responding to therapy.},
  doi        = {10.1038/clpt.2008.161},
  file       = {PubMed Central Link:https\://www.ncbi.nlm.nih.gov/pmc/articles/PMC4938843/:text/html},
  pmcid      = {PMC4938843},
  pmid       = {18754000},
  shorttitle = {The {Reference} {Image} {Database} to {Evaluate} {Response} to {Therapy} in {Lung} {Cancer} ({RIDER}) {Project}},
  url        = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4938843/},
  urldate    = {2022-04-04},
}

@Article{c43,
  author     = {Kleesiek, Jens and Urban, Gregor and Hubert, Alexander and Schwarz, Daniel and Maier-Hein, Klaus and Bendszus, Martin and Biller, Armin},
  journal    = {NeuroImage},
  title      = {Deep {MRI} brain extraction: {A} {3D} convolutional neural network for skull stripping},
  year       = {2016},
  issn       = {1095-9572},
  month      = apr,
  pages      = {460--469},
  volume     = {129},
  abstract   = {Brain extraction from magnetic resonance imaging (MRI) is crucial for many neuroimaging workflows. Current methods demonstrate good results on non-enhanced T1-weighted images, but struggle when confronted with other modalities and pathologically altered tissue. In this paper we present a 3D convolutional deep learning architecture to address these shortcomings. In contrast to existing methods, we are not limited to non-enhanced T1w images. When trained appropriately, our approach handles an arbitrary number of modalities including contrast-enhanced scans. Its applicability to MRI data, comprising four channels: non-enhanced and contrast-enhanced T1w, T2w and FLAIR contrasts, is demonstrated on a challenging clinical data set containing brain tumors (N=53), where our approach significantly outperforms six commonly used tools with a mean Dice score of 95.19. Further, the proposed method at least matches state-of-the-art performance as demonstrated on three publicly available data sets: IBSR, LPBA40 and OASIS, totaling N=135 volumes. For the IBSR (96.32) and LPBA40 (96.96) data set the convolutional neuronal network (CNN) obtains the highest average Dice scores, albeit not being significantly different from the second best performing method. For the OASIS data the second best Dice (95.02) results are achieved, with no statistical difference in comparison to the best performing tool. For all data sets the highest average specificity measures are evaluated, whereas the sensitivity displays about average results. Adjusting the cut-off threshold for generating the binary masks from the CNN's probability output can be used to increase the sensitivity of the method. Of course, this comes at the cost of a decreased specificity and has to be decided application specific. Using an optimized GPU implementation predictions can be achieved in less than one minute. The proposed method may prove useful for large-scale studies and clinical trials.},
  doi        = {10.1016/j.neuroimage.2016.01.024},
  file       = {PubMed entry:http\://www.ncbi.nlm.nih.gov/pubmed/26808333:text/html},
  keywords   = {Brain Neoplasms, Humans, Image Enhancement, Image Interpretation, Computer-Assisted, Imaging, Three-Dimensional, Machine Learning, Magnetic Resonance Imaging, Neural Networks, Computer, Neuroimaging, Skull, Brain extraction, Brain mask, Convolutional networks, Deep learning, MRI, Skull stripping},
  language   = {eng},
  pmid       = {26808333},
  shorttitle = {Deep {MRI} brain extraction},
}

@Book{c17,
  author    = {Wang, Ge and Zhang, Yi and Ye, Xiaojing and Mou, Xuanqin},
  publisher = {IOP Publishing},
  title     = {Machine Learning for Tomographic Imaging},
  year      = {2019},
  isbn      = {978-0-7503-2216-4},
  month     = dec,
  series    = {2053-2563},
  abstract  = {The area of machine learning, especially deep learning, has exploded in recent years, producing advances in everything from speech recognition and gaming to drug discovery. Tomographic imaging is another major area that is being transformed by machine learning, and its potential to revolutionise medical imaging is highly significant. Written by active researchers in the field, Machine Learning for Tomographic Imaging presents a unified overview of deep-learning-based tomographic imaging. Key concepts, including classic reconstruction ideas and human vision inspired insights, are introduced as a foundation for a thorough examination of artificial neural networks and deep tomographic reconstruction. X-ray CT and MRI reconstruction methods are covered in detail, and other medical imaging applications are discussed as well. An engaging and accessible style makes this book an ideal introduction for those in applied disciplines, as well as those in more theoretical disciplines who wish to learn about application contexts. Hands-on projects are also suggested, and links to open source software, working datasets, and network models are included. Part of Series in Physics and Engineering in Medicine and Biology.},
  doi       = {10.1088/978-0-7503-2216-4},
  ean       = {9780750322164},
  language  = {en},
  pagetotal = {250},
  url       = {https://dx.doi.org/10.1088/978-0-7503-2216-4},
  urldate   = {2022-04-04},
}

@InProceedings{c19,
  author    = {Yang, Dong and Xu, Daguang and Zhou, S. Kevin and Georgescu, Bogdan and Chen, Mingqing and Grbic, Sasa and Metaxas, Dimitris and Comaniciu, Dorin},
  booktitle = {Medical Image Computing and Computer Assisted Intervention - MICCAI 2017},
  title     = {Automatic Liver Segmentation Using an Adversarial Image-to-Image Network},
  year      = {2017},
  editor    = {Descoteaux, Maxime and Maier-Hein, Lena and Franz, Alfred and Jannin, Pierre and Collins, D. Louis and Duchesne, Simon},
  pages     = {507--515},
  publisher = {Springer International Publishing},
  abstract  = {Automatic liver segmentation in 3D medical images is essential in many clinical applications, such as pathological diagnosis of hepatic diseases, surgical planning, and postoperative assessment. However, it is still a very challenging task due to the complex background, fuzzy boundary, and various appearance of liver. In this paper, we propose an automatic and efficient algorithm to segment liver from 3D CT volumes. A deep image-to-image network (DI2IN) is first deployed to generate the liver segmentation, employing a convolutional encoder-decoder architecture combined with multi-level feature concatenation and deep supervision. Then an adversarial network is utilized during training process to discriminate the output of DI2IN from ground truth, which further boosts the performance of DI2IN. The proposed method is trained on an annotated dataset of 1000 CT volumes with various different scanning protocols (e.g., contrast and non-contrast, various resolution and position) and large variations in populations (e.g., ages and pathology). Our approach outperforms the state-of-the-art solutions in terms of segmentation accuracy and computing efficiency.},
  doi       = {10.1007/978-3-319-66179-7_58},
  file      = {Semantic Scholar Link:https\://www.semanticscholar.org/paper/Automatic-Liver-Segmentation-Using-an-Adversarial-Yang-Xu/8b6b0c1139a3ab405e6df58aaa79305867f453ef:text/html;Full Text PDF:https\://arxiv.org/pdf/1707.08037.pdf:application/pdf},
  isbn      = {978-3-319-66179-7},
  journal   = {MICCAI},
  url       = {https://doi.org/10.1007/978-3-319-66179-7_58},
}

@InProceedings{Vaswani2017,
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {Attention is {All} you {Need}},
  year      = {2017},
  publisher = {Curran Associates, Inc.},
  volume    = {30},
  abstract  = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  file      = {Full Text PDF:https\://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf:application/pdf},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate   = {2022-04-10},
}

@Article{Yang2021,
  author     = {Yang, Jiancheng and Shi, Rui and Wei, Donglai and Liu, Zequan and Zhao, Lin and Ke, Bilian and Pfister, Hanspeter and Ni, Bingbing},
  journal    = {arXiv:2110.14795 [cs, eess]},
  title      = {{MedMNIST} v2: {A} {Large}-{Scale} {Lightweight} {Benchmark} for {2D} and {3D} {Biomedical} {Image} {Classification}},
  year       = {2021},
  month      = oct,
  note       = {arXiv: 2110.14795},
  abstract   = {We introduce MedMNIST v2, a large-scale MNIST-like dataset collection of standardized biomedical images, including 12 datasets for 2D and 6 datasets for 3D. All images are pre-processed into a small size of 28x28 (2D) or 28x28x28 (3D) with the corresponding classification labels so that no background knowledge is required for users. Covering primary data modalities in biomedical images, MedMNIST v2 is designed to perform classification on lightweight 2D and 3D images with various dataset scales (from 100 to 100,000) and diverse tasks (binary/multi-class, ordinal regression, and multi-label). The resulting dataset, consisting of 708,069 2D images and 10,214 3D images in total, could support numerous research / educational purposes in biomedical image analysis, computer vision, and machine learning. We benchmark several baseline methods on MedMNIST v2, including 2D / 3D neural networks and open-source / commercial AutoML tools. The data and code are publicly available at https://medmnist.com/.},
  annote     = {Comment: The data and code are publicly available at https://medmnist.com/. arXiv admin note: text overlap with arXiv:2010.14925},
  file       = {arXiv Fulltext PDF:https\://arxiv.org/pdf/2110.14795.pdf:application/pdf},
  keywords   = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
  shorttitle = {{MedMNIST} v2},
  url        = {http://arxiv.org/abs/2110.14795},
  urldate    = {2022-04-10},
}

@InProceedings{Glorot2010,
  author    = {Glorot, Xavier and Bengio, Yoshua},
  title     = {Understanding the difficulty of training deep feedforward neural networks},
  year      = {2010},
  month     = mar,
  pages     = {249--256},
  publisher = {JMLR Workshop and Conference Proceedings},
  abstract  = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  file      = {Full Text PDF:http\://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf:application/pdf},
  issn      = {1938-7228},
  language  = {en},
  url       = {https://proceedings.mlr.press/v9/glorot10a.html},
  urldate   = {2022-04-10},
}

@Comment{jabref-meta: databaseType:bibtex;}
